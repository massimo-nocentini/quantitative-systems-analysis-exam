% article example for classicthesis.sty
\documentclass[10pt,a4paper]{article}
\usepackage{lipsum}
\usepackage{url}
\usepackage[nochapters]{classicthesis} % nochapters
% \usepackage[showframe=true]{geometry}  
\usepackage{changepage}

\begin{document}
\title{\rmfamily\normalfont\spacedallcaps{Homework for Quantitative
    Systems Analysis Exam}} \author{\spacedlowsmallcaps{Massimo
    Nocentini}\\\spacedlowsmallcaps{ 5422207, Group 1}} \date{\today}
    
    \maketitle

    %% \noindent\lipsum[1] Just a test.\footnote{This is a footnote.}      
    \begin{abstract}
      This short article collects the work I did in order to support
      my Quantitative Systems Analysis exam. The goal is to study
      resources usage respect a collection of user profiles, each of
      them catch a way of using some applications of interest in
      different time windows. I performed some experiments collecting
      results in a OLAP ``star''-like relational schema in order to
      provide summaries for dimensions under study, such as bandwidth
      usage, network activity, CPU and memory consumption.
    \end{abstract}
       
    \tableofcontents
   
    % \section*{Introduction}

    % This homework aims to study resources usage respect a collection
    % of user profiles; we're mainly interested on network activities,
    % but we pair it with CPU and memory consumption. In order to
    % complete this work we take it apart, discussing the activity plan,
    % the instrumentation of the system used by different profiles, the
    % plan of experiments and finally data management and analysis of
    % results in the following sections. In the 
 
    \newpage
    \section{Analysis Plan}
    In this section I tackle the problem at hand describing the
    methodology used to perform the requested analysis, breaking the
    problem statement down in its major components. In the next
    section I'll describe them informally, leaving a precise
    description of their actions in \autoref{sec:plan-of-experiment}.


    \subsection{User profiles}
    A central concept is that of \emph{user profile} which abstract
    the behavior of a person using a computer. In this homework I've
    considered three user profiles: a \emph{landlady}, a
    \emph{computer scientist} and a \emph{Free Software Foundation
      supporter}. Informally, a landlady will use the computer for
    simple tasks such as checking emails, surfing the web and watch
    some YouTube videos; on the other hand a computer scientist will
    use the computer for writing \TeX documents, programming (in
    Smalltalk especially), collaborate with colleagues via instant
    messaging and checking its email account. Finally a FSF supporter
    just makes available its bandwidth supplying Ubuntu like distros
    via Torrent protocol.

    \subsection{Different workloads in different time windows}
    In order to study the user profiles I arranged that each one of
    them performs their activities repeating the same sequence of
    actions (with some minor variants) in three different time
    windows, namely in the morning, in the afternoon and in the
    evening. This allow us to characterize the same activity in
    different time windows with different workloads, in order to
    compare them in a quite real scenario.

    For example the \emph{landlady} user profile will do a quick check
    of emails in the morning, writing short messages to her friends in
    the afternoon while in the evening she replies to important mails
    with more care, preparing some drafts of her reply.  On the other
    side, the \emph{computer scientist} writes articles in the
    morning, programs in the middle of the day and reviews the morning
    work in the evening.  Finally, the \emph{FSF supporter} keeps its
    Bit Torrent client open for the entire day, switching the distros
    image supplied between morning, afternoon and evening.

    It is important to observe that each activity isn't performed in
    solitude, that is both email checking and programming could happen
    concurrently with other actions, for example searching some
    information on the web. This allow us to reify a scenario that is
    close to a real context, with the drawback of losing repetition
    among experiments execution.

    \subsection{Quantities to assess}
    We are interested on the resource consumption for each user
    profile during the three time windows as explained in the previous
    section. In particular we identifies the following quantities to
    be relevant for our analysis:
    \begin{description}
    \item[memory] the amount of free, buffered and cached memory;
    \item[disk IO] the blocks received from and sent to a block
      device;
    \item[CPU] the time spent running non-kernel code, kernel code and
      idle cycles;
    \item[network traffic] incoming and outgoing packets through a
      network interface, in particular we perform a complete packet
      sniffing in order to aggregate results as explained in
      \autoref{sec:data-management-result-analysis}.
    \end{description}


    \subsection{Factors and experiment conditioning}
    Since we would like to perform a monitoring of actions that
    actually happened, we fix as ``held-constant factors'' the use of
    applications as defined in \autoref{sec:plan-of-experiment}, while
    as ``allowed-to-vary factors'' every little deviations and
    ``micro'' actions from the main line experiment description. 

    This implies that experiment definition be described proposing an
    ``average'' behavior, allowing us to study the ``confirmation'' of
    measured quantities respect the same activity; the ``discovery''
    of patterns under slightly different operational conditions and
    ``stability'' of collected measure.

    \section{Measuring system instrumentation}
    To collect data about quantities of interest we've instrumented a
    laptop (from now on we call it the ``instrumented system'' or just
    ``system'') used by user profiles. Since our study is a kind of
    monitoring, we do not need to setup neither hardware/software
    probes nor failure injectors.

    The system have a Core i7 64bit Intel architecture, equipped with
    8 GB of RAM and powered by Ubuntu Linux 14.04 LTS distribution. In
    next sections we describe two tools we've used to collect
    measures.

    \subsection{vmstat} 
    To measure memory and CPU usage, we've used the \emph{vmstat}
    \cite{vmstat} command line tool: it is very lightweight and
    releases summary lines containing a super set of requested
    quantities. In particular the command we've used is the following:
\begin{verbatim}
vmstat <sec> -n -S M 
\end{verbatim}
    just a few explanations: every $sec$ seconds it drops a summary
    line on the standard output, ``-n'' doesn't repeat header lines
    periodically, ``-S M'' use Megabytes as measure unit for memory
    quantities. Usually we redirect the printed output to a file in
    order to post process data.

    While repeating experiments we change $sec$ respect to duration of
    the experiment at hand, that is for ``short'' experiments we need
    more granularity, hence we use $sec = 1$ (this is the case for
    \emph{experiment one} regarding \emph{landlady} user profile). On
    the other hand, we increase it for ``long'' experiment (this is
    the case for experiments regarding \emph{FSF supporter} user
    profile).

    \subsection{SNORT} 
    To measure network traffic we've used the \emph{SNORT} tool
    \cite{SNORT}, as required by homework assignment. We used
    \emph{SNORT} to save \emph{all} packages, in their complete
    structure (ie, including headers and payloads) saving them in
    binary files.  So, according the manual \cite{SNORT-manual}, we
    sniffed packages with the following command:
\begin{verbatim}
sudo snort -l <log-directory>/ -b
\end{verbatim}
    In order to analyze the collected data, we access informations
    saved binary file with the ``read'' feature provided by
    \emph{SNORT} itself, redirecting the output into a plain text file
    on which we run a ``cleaning'' \emph{sed}\cite{sed} script which
    transform it into another plain text file containing a row for
    each incoming/outgoing network package, hence no SNORT custom rule
    has been used. We value this approach because it allow us to have
    a \emph{complete} history of what happened, implementing the
    processing phase in Smalltalk as described in
    \autoref{sec:data-management-result-analysis}

    \newpage
    \section{Plan of experiments}
    \label{sec:plan-of-experiment}
    In this section we plan the experiments to gain insight about
    resource usage by different user profiles, describing experiments
    for each profile in a dedicated subsection and the motivations
    underlying each plan.

    \subsection{Landlady experiments plan}

    \subsubsection*{Experiments design }
    We created the following experiments in order to stress the
    activity of mail checking in particular, since this is the main
    task performed by our landlady user profile. Here we're not very
    interested on CPU and memory consumption, instead we remarkably
    vary the workloads to see changes in network traffic: especially,
    in \emph{experiment two} we pair writing short messages with
    watching a YouTube video, while in \emph{experiment three} we
    perform drafts preparation using Google web mail interface in
    order to study the impact of automatic saving policy.

    \emph{Experiment two} has an additional video watching action just
    to differentiate substantially from \emph{experiment one}, since
    our goal here is to compare data relative to the \emph{same}
    experiment collected in \emph{different} days, instead of
    comparing data relative to \emph{different} experiments collected
    in the \emph{same} day.

    \subsubsection*{Experiments descriptions}
    
    \begin{description}
    \item[experiment one] In the morning, the landlady open Mozilla
      Firefox browser, log in into her \emph{GMail} account, checks for
      new mail and log out after reading new messages, under the
      assumption that she reads at least two messages.
    \item[experiment two] In the afternoon, the landlady open Mozilla
      Firefox browser, log in into her \emph{GMail} account, checks
      for new mail and writes three short greetings to her
      friends. Here by ``short greetings'' we mean a message with at
      most 50 words.  While writing greetings, she watches her
      favorite YouTube video
      \footnote{\url{https://www.youtube.com/watch?v=yaaoEyWQ6eI}} for
      about two minutes at the 480p resolution.
    \item[experiment three] In the evening, the landlady open Mozilla
      Firefox browser, log in into her \emph{GMail} account, checks for
      new mail and prepare two draft for an important reply (assuming
      that at least one message received during the day deserve
      careful response). After saving the two draft, it randomly
      select one of those and send it. Here by ``important reply'' we
      mean a message with at least 120 words.
    \end{description}

    \newpage
    \subsection{Computer scientist experiments plan}

    \subsubsection*{Experiments design }
    We created the following experiments in order to observe resources
    consumption of multiple applications running at the same time, in
    particular \emph{experiments one} and \emph{four} focus on email
    checking while typesetting a document; \emph{experiment two} and
    \emph{three} focus on programming and video conference (partly
    instant messaging).

    In each experiment we couple an application that produce network
    traffic with one that consume CPU and memory, in order to have a
    profile quite opposite to that of \emph{landlady}. In this setting
    would make sense compare \emph{different} experiments given that
    they share the same activities.

    Particular interest can be pointed to \emph{experiment two} where
    we can study (small) network traffic during an instant messaging
    session between two peers while doing a computation intensive
    task. On the other hand in \emph{experiment three} we can observe
    (huge) network traffic generated by a Google Hangout video call
    while running a process with low priority.

    \subsubsection*{Experiments descriptions}
    
    \begin{description}
    \item[experiment one] In the early morning, the computer scientist
      open the Mozilla Firefox browser, logs in into his GMail account
      and do a quick check for new messages; we assume that he reads
      at least one message. Then, keeping the browser running, he
      writes a draft of at least 400 words using the Emacs text
      editor, compiles it using \TeX three times. Then he writes one
      message to his advisor with the compiled draft as
      attachment. Finally he logs out from Google web mail interface.
    \item[experiment two] In the late morning, the computer scientist
      do some hacking on his Pharo Smalltalk system, executing an
      intensive fluid dynamics analysis for a gas network. This
      analysis is repeated three times, each of one is 20 seconds
      longer. Waiting for analysis results, he talks with his
      colleague using Google Hangout directly from his GMail web mail
      interface, sending and receiving at least 10 short phrases. Here
      by ``short phrase'' we mean a message with at most 10 words.
    \item[experiment three] In the afternoon, the computer scientist
      discuss the fluid analysis result obtained in the morning in a
      joint work through a video call using Google Hangout. The call
      has duration about one minute, during which he keeps the Pharo
      Smalltalk image running but he do not apply any change.
    \item[experiment four] In the evening, the computer scientist
      reviews the draft written in the early morning, using the Emacs
      text editor to skim through it and applies a spell checker. He
      at least recompile the document two times and after that he
      dispatch three mails from his Google web mail interface to his
      close friends. We assume that such minor modifications do not
      change significantly the compiled file dimension.
    \end{description}

    \newpage
    \subsection{FSF supporter experiment plan}

    \subsubsection*{Experiments design }
    We created the following experiments in order to observe exactly
    one activity respect network traffic and disk IO. Both three
    experiments focus on the Transmission client for BitTorrent
    protocol supplying, without any limit on upstream bandwidth, three
    different Ubuntu-like distro images, one for each experiment.

    This settings allow a comparison of both the \emph{same}
    experiment among \emph{different} repetitions, both
    \emph{different} experiments respect the \emph{same} repetition.

    As a side note, this allow us to make a comparison of the traffic
    generated by the three Ubuntu-like distros.

    \subsubsection*{Experiments descriptions}
    
    \begin{description}
    \item[experiment one] In the morning, the FSF supporter opens the
      Bit Torrent Transmission client application in order to supply
      bandwidth providing latest Kubuntu Linux ISO image
      distribution. He doesn't set any bandwidth upload limit and
      keeps Transmission running for about two minutes.
    \item[experiment two] In the afternoon, the FSF supporter opens
      the Bit Torrent Transmission client application in order to
      supply bandwidth providing latest Ubuntu Linux ISO image
      distribution. He doesn't set any bandwidth upload limit and
      keeps Transmission running for about two minutes.
    \item[experiment three] In the evening, the FSF supporter opens
      the Bit Torrent Transmission client application in order to
      supply bandwidth providing latest Xubuntu Linux ISO image
      distribution. He doesn't set any bandwidth upload limit and
      keeps Transmission running for about two minutes.
    \end{description}

    \subsection{Execution methodology}

    Given an user profile, we repeat each experiment for five
    consecutive days, ensuring that each repetition happen within time
    window as required by experiment description, even though instant
    timings can varies among repetitions of the same experiment given
    two different days. In this work we refer to days in which
    experiment repetitions happen as \emph{Day one}, \emph{Day two},
    \emph{Day three}, \emph{Day four} and \emph{Day five},
    respectively.

    \newpage
    \section{Data management and analysis of results}
    \label{sec:data-management-result-analysis}

    In this section we'll describe the software architecture
    developed, dealing with it in the former part and we give our
    results interpretation in the latter one.

    \subsection{Software architecture}
    We developer a quite rich architecture to support our results
    interpretation. We would like to have an automatic tool that
    parses Snort log files, builds a comprehensive facts collection
    and supplies a fancy and flexible interface (programmatically, of
    course) to plot quantities of interests, exporting automatically
    in .eps files, and to aggregate measures into a summary, exporting
    automatically in \TeX tabular files. In the following we provide a
    brief description of this piece of work.

    \subsubsection{Pharo Smalltalk programming environment and TDD}
    n
    We implemented our code in Pharo Smalltalk, an image based
    programming environment. It allow a natural Test Driven
    Development style, so we strictly follow it. In what follows we
    refer to classes we've implemented and are available in the image,
    assuming the curious reader has set up the enviroment correctly
    (see \autoref{sec:appendix} for instructions).

    The class \emph{AQSLogFileTest} is the test suite we've created to
    drive our implementation: it contains test methods that assert on
    log file parsing, facts collecting, plotting and summary
    generation.

    \subsubsection{Snort log files parsing}

    To set the stage we need to handle binary Snort log file in a more
    comfortable way. We saved original Snort binary log files in the
    file system, organizing them hierarchically, by experiment number,
    user profile and day number, in the given order. Hence we process
    every log file by performing a depth-first visit, applying to each
    one of them a bash script which does some clean work, especially
    removing empty and separating lines, joining the rest toward the
    creation of new log file which has exactly one line per packet;
    here we report a chunk of
    the latter log (intentionally breaking page boundaries): 
    \begin{adjustwidth}{-4cm}{}
\begin{verbatim}
07/01-07:41:07.220098 92.104.129.98:6881 -> 192.168.0.4:51413 UDP TTL:45 TOS:0x0 ID:0 IpLen:20 DgmLen:129 DF
07/01-07:41:07.220190 192.168.0.4 -> 92.104.129.98 ICMP TTL:64 TOS:0xC0 ID:48370 IpLen:20 DgmLen:157 Type:3  Code:3  DESTINATION UNREACHABLE: PORT UNREACHABLE
07/01-07:41:07.277850 98.232.94.206:59805 -> 192.168.0.4:51413 TCP TTL:112 TOS:0x0 ID:14283 IpLen:20 DgmLen:48 DF ******S* Seq: 0x709F7FB  Ack: 0x0  Win: 0x2000  TcpLen: 28
07/01-07:41:07.277898 192.168.0.4:51413 -> 98.232.94.206:59805 TCP TTL:64 TOS:0x0 ID:41997 IpLen:20 DgmLen:40 DF ***A*R** Seq: 0x0  Ack: 0x709F7FC  Win: 0x0  TcpLen: 20
07/01-07:41:07.528356 91.65.105.145:51413 -> 192.168.0.4:51413 UDP TTL:47 TOS:0x0 ID:32138 IpLen:20 DgmLen:58 D
\end{verbatim}
    \end{adjustwidth}

    Performing this cleaning allow to handle files much shorter than
    the original ones produced by the ``read'' Snort feature: for
    example, one of our files shrinks from about 70000 to 14000 lines.

    \subsubsection{Building facts collection}

    Following \cite{bondavalli}, we introduced the concept of
    \emph{fact}, which collect all relevant information about events
    we care about. We do not use a database backend, preferring live
    objects to talk with. This allow an interactive session where the
    user can query the facts collection using all the pretty stuff
    supplied by Pharo Smalltalk to inspect and explore them, for
    example it is possible to filter facts using a predicate and use
    that selection programmatically. This methodology has the drawback
    to be slower respect a relational database engine, while gaining
    in flexibility.

    \subsubsection{Selecting facts toward actions}
    
    We use the facts collection to define a selection over it,
    reifying this concept in its own dedicate class
    \emph{FactsSelection}. This allow us to use the selection as a
    ``trampoline'' object \cite{weiher-ducasse} toward actions, from
    our point of view, plotting and summarizing. This allows
    extensibility since if a developer would like to use facts for a
    different goal, let say drawing histograms, it is required only to
    implement an action object able to receive a selection of
    facts. The two action classes relative to plotting and summarizing
    are \emph{FactsPlotter} and \emph{FactsSummary}, respectively. 

    It is possible to create a selection of fact providing a user
    profile, an experiment number and a day, additionally to an
    arbitrary predicate.

    \subsubsection{Plotting a facts selection}

    We supply one implementation for plotting a facts selection, with
    two main features: the one allow to scatter a quantity of interest
    agains time instant, the other allow to partition a quantity by
    another fact property, associating a different formatting for each
    group. We've used the former to scatter the CPU usage, while the
    latter to partition packet length by protocol packet membership,
    just for an use case. It is interesting to observe that both
    quantity selection both partitioning is implemented via an high
    order system, that is the user can select \emph{any} quantity
    using a declarative style, nothing is hardcoded for the problem at
    hand. From another point of view, if another Snort log file is
    under study, with different properties than ours, it is possible
    to use this plotting mechanisms just plugging in a block that
    selects the quantity it is required to scatter (of course, the
    data series format is customizable too). As plotting backend we
    lie on \emph{GnuPlot} \cite{gnuplot}.

    \subsubsection{Summarizing a facts selection}

    We supply one implementation for summarizing a facts selection,
    with a main feature to aggregate facts along some dimensions, an
    example can be see in tables reported in the following results
    section. Here the main issue is, given an user profile, build a
    matrix which has days on rows and experiments on columns. For each
    fact we assign it to a matrix cell, in order to post process each
    cell depending on its data content. If some data are available,
    the job is done by class \emph{DatafulCellStatus}, which interpret
    facts measures to supply the required aggregated values.

    As the case for plotting, if a user would like to aggregate
    respect different dimensions, it is required to specialize
    \emph{DatafulCellStatus} to have an output formatted as \TeX
    tabular environment.

    \subsubsection{Putting it all together}
    
    To make a long business short, we've implemented a set of classes
    whose responsibility is to interpret a log file, providing
    extension points to make a facts selection and using it for
    plotting against quantities of interest, which can seen as a whole
    or as a partition; or for summarizing producing a \TeX file
    containing a tabular environment region ready to be used as input
    file in an bigger document (as we're doing while writing this
    article).
    
    \newpage

    \subsection{Results commentary}

    \subsubsection{Landlady user profile}
    

    \subsubsection{Computer scientist user profile}


    \subsubsection{FSF supporter user profile}

    
    \subsubsection{Summary tables}
    \input{summary-Landlady.tex}
    \input{summary-ComputerScientist.tex}    
    \input{summary-FSFSupporter.tex}
    



  %   \begin{table}
  %     \begin{adjustwidth}{-2cm}{}
  %     \centering
  %     \begin{tabular}{l| l| l | l| l }
  %       & Experiment 1& Experiment 2& Experiment 3& Experiment 4\\
  %       \hline
  %     Day 1 &
  %     \begin{tabular}{ l  r }
  %       cpu idle & 96.1 \% \\
  %       cpu user& 2\%  \\
  %       cpu kernel & 1.1 \% \\
  %       free mem & 4.3 GB \\
  %       \hline
  %       num packets & 9283 \\
  %       dim sum & 1.5 MB \\
  %       packets/sec & 6 \\
  %       interlap & 6 secs \\
  %       UDP & 6 \\
  %       TCP msgs & 6\% \\
  %       dim/msg & 146.3 Byte \\
  %     \end{tabular} &       \begin{tabular}{ l  r }
  %       cpu idle & 2  \\
  %       cpu user& 2  \\
  %       cpu kernel & 6 \\
  %       free mem & 6 \\
  %       num packets & 6 \\
  %       dim sum & 6 \\
  %       packets/sec & 6 \\
  %     \end{tabular} &       \begin{tabular}{ l  r }
  %       cpu idle & 2  \\
  %       cpu user& 2  \\
  %       cpu kernel & 6 \\
  %       free mem & 6 \\
  %       num packets & 6 \\
  %       dim sum & 6 \\
  %       packets/sec & 6 \\
  %     \end{tabular} &      \begin{tabular}{ l  r }
  %       cpu idle & 2  \\
  %       cpu user& 2  \\
  %       cpu kernel & 6 \\
  %       free mem & 6 \\
  %       num packets & 6 \\
  %       dim sum & 6 \\
  %       packets/sec & 6 \\       
  %     \end{tabular} \\
  %       \hline
  %     Day 1 &
  %     \begin{tabular}{ l  r }
  %       cpu idle & 2  \\
  %       cpu user& 2  \\
  %       cpu kernel & 6 \\
  %       free mem & 6 \\
  %       num packets & 6 \\
  %       dim sum & 6 \\
  %       packets/sec & 6 \\
  %       interlap & 6 \\
  %     \end{tabular} &       \begin{tabular}{ l  r }
  %       cpu idle & 2  \\
  %       cpu user& 2  \\
  %       cpu kernel & 6 \\
  %       free mem & 6 \\
  %       num packets & 6 \\
  %       dim sum & 6 \\
  %       packets/sec & 6 \\
  %     \end{tabular} &       \begin{tabular}{ l  r }
  %       cpu idle & 2  \\
  %       cpu user& 2  \\
  %       cpu kernel & 6 \\
  %       free mem & 6 \\
  %       num packets & 6 \\
  %       dim sum & 6 \\
  %       packets/sec & 6 \\
  %     \end{tabular} &      \begin{tabular}{ l  r }
  %       cpu idle & 2  \\
  %       cpu user& 2  \\
  %       cpu kernel & 6 \\
  %       free mem & 6 \\
  %       num packets & 6 \\
  %       dim sum & 6 \\
  %       packets/sec & 6 \\       
  %     \end{tabular} \\
  %       \hline
  %     Day 1 &
  %     \begin{tabular}{ l  r }
  %       cpu idle & 2  \\
  %       cpu user& 2  \\
  %       cpu kernel & 6 \\
  %       free mem & 6 \\
  %       num packets & 6 \\
  %       dim sum & 6 \\
  %       packets/sec & 6 \\
  %       interlap & 6 \\
  %     \end{tabular} &       \begin{tabular}{ l  r }
  %       cpu idle & 2  \\
  %       cpu user& 2  \\
  %       cpu kernel & 6 \\
  %       free mem & 6 \\
  %       num packets & 6 \\
  %       dim sum & 6 \\
  %       packets/sec & 6 \\
  %     \end{tabular} &       \begin{tabular}{ l  r }
  %       cpu idle & 2  \\
  %       cpu user& 2  \\
  %       cpu kernel & 6 \\
  %       free mem & 6 \\
  %       num packets & 6 \\
  %       dim sum & 6 \\
  %       packets/sec & 6 \\
  %     \end{tabular} &      \begin{tabular}{ l  r }
  %       cpu idle & 2  \\
  %       cpu user& 2  \\
  %       cpu kernel & 6 \\
  %       free mem & 6 \\
  %       num packets & 6 \\
  %       dim sum & 6 \\
  %       packets/sec & 6 \\       
  %     \end{tabular} \\
  %       \hline
  %     Day 1 &
  %     \begin{tabular}{ l  r }
  %       cpu idle & 2  \\
  %       cpu user& 2  \\
  %       cpu kernel & 6 \\
  %       free mem & 6 \\
  %       num packets & 6 \\
  %       dim sum & 6 \\
  %       packets/sec & 6 \\
  %       interlap & 6 \\
  %     \end{tabular} &       \begin{tabular}{ l  r }
  %       cpu idle & 2  \\
  %       cpu user& 2  \\
  %       cpu kernel & 6 \\
  %       free mem & 6 \\
  %       num packets & 6 \\
  %       dim sum & 6 \\
  %       packets/sec & 6 \\
  %     \end{tabular} &       \begin{tabular}{ l  r }
  %       cpu idle & 2  \\
  %       cpu user& 2  \\
  %       cpu kernel & 6 \\
  %       free mem & 6 \\
  %       num packets & 6 \\
  %       dim sum & 6 \\
  %       packets/sec & 6 \\
  %     \end{tabular} &      \begin{tabular}{ l  r }
  %       cpu idle & 2  \\
  %       cpu user& 2  \\
  %       cpu kernel & 6 \\
  %       free mem & 6 \\
  %       num packets & 6 \\
  %       dim sum & 6 \\
  %       packets/sec & 6 \\       
  %     \end{tabular} \\
  %       \hline
  %     Day 1 &
  %     \begin{tabular}{ l  r }
  %       cpu idle & 2  \\
  %       cpu user& 2  \\
  %       cpu kernel & 6 \\
  %       free mem & 6 \\
  %       num packets & 6 \\
  %       dim sum & 6 \\
  %       packets/sec & 6 \\
  %       interlap & 6 \\
  %     \end{tabular} &       \begin{tabular}{ l  r }
  %       cpu idle & 2  \\
  %       cpu user& 2  \\
  %       cpu kernel & 6 \\
  %       free mem & 6 \\
  %       num packets & 6 \\
  %       dim sum & 6 \\
  %       packets/sec & 6 \\
  %     \end{tabular} &       \begin{tabular}{ l  r }
  %       cpu idle & 2  \\
  %       cpu user& 2  \\
  %       cpu kernel & 6 \\
  %       free mem & 6 \\
  %       num packets & 6 \\
  %       dim sum & 6 \\
  %       packets/sec & 6 \\
  %     \end{tabular} &      \begin{tabular}{ l  r }
  %       cpu idle & 2  \\
  %       cpu user& 2  \\
  %       cpu kernel & 6 \\
  %       free mem & 6 \\
  %       num packets & 6 \\
  %       dim sum & 6 \\
  %       packets/sec & 6 \\       
  %     \end{tabular} \\
  %   \end{tabular}
  % \end{adjustwidth}
  % \end{table}
    \newpage

    \section{Appendix}
    \label{sec:appendix}

    \subsection{License}
\begin{verbatim}
The MIT License (MIT)

Copyright (c) 2014 Massimo Nocentini

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
\end{verbatim}

    \subsection{Project hosting}

    \subsection{Environment setup}

    

    \begin{thebibliography}{9}

    \bibitem{SNORT}
      SNORT Team,
      \url{https://www.snort.org/}.

    \bibitem{SNORT-manual}
      SNORT Team,
      \url{http://manual.snort.org/}.

    \bibitem{vmstat}
      Free Software Foundation,
      \url{http://www.freebsd.org/cgi/man.cgi?query=vmstat}.

    \bibitem{sed}
      Free Software Foundation,
      \url{http://www.freebsd.org/cgi/man.cgi?query=sed}

    \bibitem{gnuplot} Free Software
      Foundation,\url{http://www.gnuplot.info/}

    \bibitem{bondavalli} Andrea Bondavalli, Analisi quantitativa dei
      sistemi critici, March 2011, Progetto Leonardo, Esculapio
      Editore

    \bibitem{weiher-ducasse} Marcel Weiher and Stephane Ducasse,
      Higher Order Messaging, Dynamic Languages Symposium (DLS) '05,
      October 18, 2005, San Diego, CA, USA


    \end{thebibliography}
    % % bib stuff
    % \nocite{*}
    % \addtocontents{toc}{\protect\vspace{\beforebibskip}}
    % \addcontentsline{toc}{section}{\refname}    
    % \bibliographystyle{plain}
    % \bibliography{../Bibliography}
\end{document}
